{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.som import SOM\n",
    "from modules.model_picker import model_picker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from modules.utils import euc_distance\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 4), (150,))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data.csv\", header=None)\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:,-1].values\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize a column of data\n",
    "def normalize_column(data, column_index):\n",
    "    column = data[:, column_index]\n",
    "    min_val = np.min(column)\n",
    "    max_val = np.max(column)\n",
    "    normalized_column = (column - min_val) / (max_val - min_val)\n",
    "    return normalized_column\n",
    "\n",
    "\n",
    "class som_classification:\n",
    "    def __init__(self, m:int, n:int, X:np.array, y: np.array,) -> None:\n",
    "        # preprocess X\n",
    "        # Normalize each column\n",
    "        for col in range(X.shape[1]):\n",
    "            X[:, col] = normalize_column(X, col)\n",
    "        y = np.reshape(y, -1)\n",
    "        self.total_neurons_representation = m*n\n",
    "        self.neuron_shape = (m,n)\n",
    "        self.classes = np.unique(y)\n",
    "        self.dataset = []\n",
    "        self.all_dataset = X\n",
    "        self.true = y\n",
    "        self.true_encoded = one_hot_encode(y)\n",
    "        self.best_models = None\n",
    "        self.trained = False\n",
    "        for c in self.classes:\n",
    "            X_filtered = X[(y == c)]\n",
    "            self.dataset.append((X_filtered, c))\n",
    "\n",
    "    \n",
    "    def predict(self, X: np.array):\n",
    "        predictions = []\n",
    "\n",
    "        for data in X:\n",
    "            list_samples = [\n",
    "                model.neurons[model.index_bmu(np.array([data]))[0]][model.index_bmu(np.array([data]))[1]]\n",
    "                for model in self.models\n",
    "            ]\n",
    "            #print(self.models)\n",
    "            # Calculate distances and sum\n",
    "            distances = [math.exp(1 / (euc_distance(data, sample) + 1)) for sample in list_samples]\n",
    "            dist_sum = sum(distances)\n",
    "            \n",
    "            # Normalize distances\n",
    "            normalized_distances = [dist / dist_sum for dist in distances]\n",
    "            \n",
    "            predictions.append(normalized_distances)\n",
    "        \n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def fit(self, epoch: int=10):\n",
    "        # find the total classes\n",
    "        \n",
    "        if not self.trained:\n",
    "            self.trained = True\n",
    "            self.models = []\n",
    "            for data_points, label in self.dataset:\n",
    "                som = SOM(m=self.neuron_shape[0], n=self.neuron_shape[1], dim=X.shape[1], \n",
    "                        initiate_method='SOM++', learning_rate=1.5, \n",
    "                        neighbour_rad=2.0, distance_function='euclidean', max_iter=None)\n",
    "                som.fit(X = data_points, epoch=epoch, verbose=False)\n",
    "                self.models.append(som)\n",
    "        else:\n",
    "            for datasets, som_model in zip(self.dataset, self.models):\n",
    "                data_points, label = datasets\n",
    "                som_model.fit(X=data_points, epoch=epoch, verbose=False)\n",
    "        \n",
    "    def eval(self):\n",
    "        pred = self.predict(self.all_dataset)\n",
    "        #print(pred)\n",
    "        mean_squared_error = np.mean(np.sum((self.true_encoded - pred)**2, axis=1))\n",
    "        accuracy = np.sum([1 if self.classes[np.argmax(pred_data)] == true_data else 0 for pred_data, true_data in zip(pred, self.true)])/len(pred)\n",
    "        return (mean_squared_error, accuracy)\n",
    "    \n",
    "    def train(self, retry:int=10, train_batch:int=24):\n",
    "        self.fit(1)\n",
    "        best_mse = 1e10\n",
    "        best_acc = 0\n",
    "        while True and retry > 0:\n",
    "            mse, acc = self.eval()\n",
    "            self.fit(train_batch)\n",
    "            print(mse, acc)\n",
    "            if mse < best_mse or acc > best_acc:\n",
    "                if mse < best_mse:\n",
    "                    best_mse = mse\n",
    "                if acc > best_acc:\n",
    "                    best_acc = acc\n",
    "                self.best_models = self.models\n",
    "                retry = 10\n",
    "            else:\n",
    "                retry -= 1\n",
    "        self.models = self.best_models\n",
    "        return self.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def normalize_column(data, column_index):\n",
    "    column = data[:, column_index]\n",
    "    min_val = np.min(column)\n",
    "    max_val = np.max(column)\n",
    "    normalized_column = (column - min_val) / (max_val - min_val)\n",
    "    return normalized_column\n",
    "\n",
    "def one_hot_encode(y):\n",
    "    classes = np.unique(y)\n",
    "    encoded = np.zeros((y.size, classes.size))\n",
    "    for idx, label in enumerate(y):\n",
    "        encoded[idx, np.where(classes == label)[0][0]] = 1\n",
    "    return encoded\n",
    "\n",
    "class som_classification:\n",
    "    def __init__(self, m: int, n: int, X: np.array, y: np.array) -> None:\n",
    "        # Normalize each column\n",
    "        for col in range(X.shape[1]):\n",
    "            X[:, col] = normalize_column(X, col)\n",
    "        \n",
    "        y = np.reshape(y, -1)\n",
    "        self.total_neurons_representation = m * n\n",
    "        self.neuron_shape = (m, n)\n",
    "        self.classes = np.unique(y)\n",
    "        self.dataset = []\n",
    "        self.all_dataset = X\n",
    "        self.true = y\n",
    "        self.true_encoded = one_hot_encode(y)\n",
    "        self.best_models = None\n",
    "        self.trained = False\n",
    "        self.weights = np.random.rand(len(self.classes)).astype(np.float64)\n",
    "        for c in self.classes:\n",
    "            X_filtered = X[(y == c)]\n",
    "            self.dataset.append((X_filtered, c))\n",
    "        # Initialize weights for the models\n",
    "        self.weights = np.ones(len(self.classes))\n",
    "\n",
    "    def predict(self, X: np.array):\n",
    "        predictions = []\n",
    "\n",
    "        for data in X:\n",
    "            list_samples = [\n",
    "                model.neurons[model.index_bmu(np.array([data]))[0]][model.index_bmu(np.array([data]))[1]]\n",
    "                for model in self.models\n",
    "            ]\n",
    "            distances = [math.exp(1 / (euc_distance(data, sample) + 1)) for sample in list_samples]\n",
    "            dist_sum = sum(distances)\n",
    "            normalized_distances = np.array([dist / dist_sum for dist in distances])\n",
    "            # Apply weights to normalized distances\n",
    "            weighted_distances = [w * d for w, d in zip(self.weights, normalized_distances)]\n",
    "            predictions.append(weighted_distances)\n",
    "        \n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def update_weights(self, learning_rate=0.01, iterations=100):\n",
    "        for _ in range(iterations):\n",
    "            # Calculate predictions\n",
    "            pred = self.predict(self.all_dataset)\n",
    "            error = self.true_encoded - pred\n",
    "\n",
    "            # Calculate gradient\n",
    "            gradient = np.zeros_like(self.weights)\n",
    "            for i, (p, t) in enumerate(zip(pred, self.true_encoded)):\n",
    "                for j in range(len(self.weights)):\n",
    "                    gradient[j] += -2 * error[i][j] * p[j]\n",
    "\n",
    "            # Update weights\n",
    "            self.weights -= learning_rate * gradient\n",
    "        \n",
    "        # Normalize weights to sum to 1 (optional, depending on your requirements)\n",
    "        self.weights = self.weights / np.sum(self.weights)\n",
    "    def fit(self, epoch: int = 10, initiate_method = 'kde', learning_rate=1.5, distance_function = \"euclidean\"):\n",
    "        if not self.trained:\n",
    "            self.trained = True\n",
    "            self.models = []\n",
    "            for data_points, label in self.dataset:\n",
    "                som = SOM(m=self.neuron_shape[0], n=self.neuron_shape[1], dim=self.all_dataset.shape[1], \n",
    "                          initiate_method=initiate_method, learning_rate=learning_rate, \n",
    "                          neighbour_rad=2.0, distance_function=distance_function, max_iter=None)\n",
    "                som.fit(X=data_points, epoch=epoch, verbose=False)\n",
    "                self.models.append(som)\n",
    "        else:\n",
    "            for datasets, som_model in zip(self.dataset, self.models):\n",
    "                data_points, label = datasets\n",
    "                som_model.fit(X=data_points, epoch=epoch, verbose=False)\n",
    "        # Gradient descent to optimize weights\n",
    "        \n",
    "        # Gradient descent to optimize weights\n",
    "        self.update_weights( iterations=epoch)\n",
    "        \n",
    "    def eval(self):\n",
    "        pred = self.predict(self.all_dataset)\n",
    "        mean_squared_error = np.mean(np.sum((self.true_encoded - pred) ** 2, axis=1))\n",
    "        accuracy = np.sum([1 if self.classes[np.argmax(pred_data)] == true_data else 0 for pred_data, true_data in zip(pred, self.true)]) / len(pred)\n",
    "        return mean_squared_error, accuracy\n",
    "    \n",
    "    def train(self, retry: int = 10, train_batch: int = 24):\n",
    "        self.fit(1)\n",
    "        best_mse = float('inf')\n",
    "        best_acc = 0\n",
    "        while retry > 0:\n",
    "            mse, acc = self.eval()\n",
    "            self.fit(train_batch)\n",
    "            print(mse, acc)\n",
    "            if mse < best_mse or acc > best_acc:\n",
    "                if mse < best_mse:\n",
    "                    best_mse = mse\n",
    "                if acc > best_acc:\n",
    "                    best_acc = acc\n",
    "                self.best_models = self.models\n",
    "                retry = 10\n",
    "            else:\n",
    "                retry -= 1\n",
    "        self.models = self.best_models\n",
    "        return self.eval(), (best_mse, best_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8086177128507226 0.3333333333333333\n",
      "0.8049070224036231 0.6333333333333333\n",
      "0.7936725000983668 0.7733333333333333\n",
      "0.7840864858546823 0.8266666666666667\n",
      "0.7765698158479089 0.8866666666666667\n",
      "0.774912544893786 0.86\n",
      "0.7754221660856325 0.8933333333333333\n",
      "0.7768519694573246 0.88\n",
      "0.7772944440193776 0.8666666666666667\n",
      "0.7786481021810022 0.8933333333333333\n",
      "0.7787539278192435 0.8866666666666667\n",
      "0.7775933008965169 0.9\n",
      "0.7813669862669346 0.8933333333333333\n",
      "0.779090104822294 0.8666666666666667\n",
      "0.7801740327365236 0.8666666666666667\n",
      "0.7795755066067299 0.8733333333333333\n",
      "0.78159088981332 0.88\n",
      "0.7807210031533034 0.9066666666666666\n",
      "0.7814660805643204 0.8866666666666667\n",
      "0.7810690571659978 0.9133333333333333\n",
      "0.7805321032067873 0.9066666666666666\n",
      "0.7801789733273995 0.9066666666666666\n",
      "0.7807720544722719 0.8866666666666667\n",
      "0.7797154906162814 0.8933333333333333\n",
      "0.7787686314641731 0.8933333333333333\n",
      "0.7775278328052897 0.8866666666666667\n",
      "0.7773558810500873 0.8933333333333333\n",
      "0.7756061814866255 0.8866666666666667\n",
      "0.7755365796136762 0.92\n",
      "0.7760545027533334 0.9066666666666666\n",
      "0.775390378740608 0.92\n",
      "0.7752559292694463 0.9133333333333333\n",
      "0.77461938658119 0.8933333333333333\n",
      "0.7742935588839857 0.8933333333333333\n",
      "0.7727186787691518 0.9133333333333333\n",
      "0.7717992613159194 0.92\n",
      "0.7711976110433045 0.92\n",
      "0.7709707560664794 0.9333333333333333\n",
      "0.7711489391131983 0.9266666666666666\n",
      "0.7728609413513522 0.94\n",
      "0.7727530842528237 0.9266666666666666\n",
      "0.7716026011911864 0.92\n",
      "0.7712085468723177 0.9333333333333333\n",
      "0.7711616710271217 0.9266666666666666\n",
      "0.7719336183274891 0.9266666666666666\n",
      "0.7715160987862479 0.9\n",
      "0.7714049993390136 0.8933333333333333\n",
      "0.7709163327647971 0.8933333333333333\n",
      "0.7721284948796163 0.88\n",
      "0.7723044009232677 0.9066666666666666\n",
      "0.7724137207916675 0.8933333333333333\n",
      "0.7715426045420516 0.9133333333333333\n",
      "0.7715559765142138 0.9066666666666666\n",
      "0.7714748581217181 0.9266666666666666\n",
      "0.7710603077076726 0.9333333333333333\n",
      "0.7725744487495709 0.9266666666666666\n",
      "0.7723782503016483 0.9266666666666666\n",
      "0.7713392114206387 0.9266666666666666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((0.7718605527125989, 0.94), (0.7709163327647971, 0.94))"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "som_class = som_classification(2,2, X, y)\n",
    "#print(som_class.all_dataset)\n",
    "eval, best = som_class.train(retry=16, train_batch=128)\n",
    "eval, best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
